{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCc3XZEyG3XV"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 3, Module 1*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Define ML problems\n",
    "\n",
    "You will use your portfolio project dataset for all assignments this sprint.\n",
    "\n",
    "## Assignment\n",
    "\n",
    "Complete these tasks for your project, and document your decisions.\n",
    "\n",
    "- [ ] Choose your target. Which column in your tabular dataset will you predict?\n",
    "- [ ] Is your problem regression or classification?\n",
    "- [ ] How is your target distributed?\n",
    "    - Classification: How many classes? Are the classes imbalanced?\n",
    "    - Regression: Is the target right-skewed? If so, you may want to log transform the target.\n",
    "- [ ] Choose your evaluation metric(s).\n",
    "    - Classification: Is your majority class frequency >= 50% and < 70% ? If so, you can just use accuracy if you want. Outside that range, accuracy could be misleading. What evaluation metric will you choose, in addition to or instead of accuracy?\n",
    "    - Regression: Will you use mean absolute error, root mean squared error, R^2, or other regression metrics?\n",
    "- [ ] Choose which observations you will use to train, validate, and test your model.\n",
    "    - Are some observations outliers? Will you exclude them?\n",
    "    - Will you do a random split or a time-based split?\n",
    "- [ ] Begin to clean and explore your data.\n",
    "- [ ] Begin to choose which features, if any, to exclude. Would some features \"leak\" future information?\n",
    "\n",
    "If you haven't found a dataset yet, do that today. [Review requirements for your portfolio project](https://lambdaschool.github.io/ds/unit2) and choose your dataset.\n",
    "\n",
    "Some students worry, ***what if my model isn't “good”?*** Then, [produce a detailed tribute to your wrongness. That is science!](https://twitter.com/nathanwpyle/status/1176860147223867393)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle ML datasets (From 231 assignment)\n",
    "[ ] Continue to clean and explore your data.\n",
    "\n",
    "[ ] For the evaluation metric you chose, what score would you get just by guessing?\n",
    "\n",
    "[ ] Can you make a fast, first model that beats guessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 3, Module 3*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Permutation & Boosting\n",
    "\n",
    "You will use your portfolio project dataset for all assignments this sprint.\n",
    "\n",
    "## Assignment\n",
    "\n",
    "Complete these tasks for your project, and document your work.\n",
    "\n",
    "- [ ] If you haven't completed assignment #1, please do so first.\n",
    "- [ ] Continue to clean and explore your data. Make exploratory visualizations.\n",
    "- [ ] Fit a model. Does it beat your baseline? \n",
    "- [ ] Try xgboost.\n",
    "- [ ] Get your model's permutation importances.\n",
    "\n",
    "You should try to complete an initial model today, because the rest of the week, we're making model interpretation visualizations.\n",
    "\n",
    "But, if you aren't ready to try xgboost and permutation importances with your dataset today, that's okay. You can practice with another dataset instead. You may choose any dataset you've worked with previously.\n",
    "\n",
    "The data subdirectory includes the Titanic dataset for classification and the NYC apartments dataset for regression. You may want to choose one of these datasets, because example solutions will be available for each.\n",
    "\n",
    "\n",
    "## Reading\n",
    "\n",
    "Top recommendations in _**bold italic:**_\n",
    "\n",
    "#### Permutation Importances\n",
    "- _**[Kaggle / Dan Becker: Machine Learning Explainability](https://www.kaggle.com/dansbecker/permutation-importance)**_\n",
    "- [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
    "\n",
    "#### (Default) Feature Importances\n",
    "  - [Ando Saabas: Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
    "  - [Terence Parr, et al: Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
    "\n",
    "#### Gradient Boosting\n",
    "  - [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
    "  - [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf), Chapter 8\n",
    "  - _**[Gradient Boosting Explained](https://www.gormanalysis.com/blog/gradient-boosting-explained/)**_ — Ben Gorman\n",
    "  - [Gradient Boosting Explained](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html) — Alex Rogozhnikov\n",
    "  - [How to explain gradient boosting](https://explained.ai/gradient-boosting/) — Terence Parr & Jeremy Howard\n",
    "  \n",
    "# Model Interpretation\n",
    "\n",
    "You will use your portfolio project dataset for all assignments this sprint.\n",
    "\n",
    "## Assignment\n",
    "\n",
    "Complete these tasks for your project, and document your work.\n",
    "\n",
    "- [ ] Continue to iterate on your project: data cleaning, exploratory visualization, feature engineering, modeling.\n",
    "- [ ] Make at least 1 partial dependence plot to explain your model.\n",
    "- [ ] Make at least 1 Shapley force plot to explain an individual prediction.\n",
    "- [ ] **Share at least 1 visualization (of any type) on Slack!**\n",
    "\n",
    "If you aren't ready to make these plots with your own dataset, you can practice these objectives with any dataset you've worked with previously. Example solutions are available for Partial Dependence Plots with the Tanzania Waterpumps dataset, and Shapley force plots with the Titanic dataset. (These datasets are available in the data directory of this repository.)\n",
    "\n",
    "Please be aware that **multi-class classification** will result in multiple Partial Dependence Plots (one for each class), and multiple sets of Shapley Values (one for each class).\n",
    "\n",
    "## Stretch Goals\n",
    "\n",
    "#### Partial Dependence Plots\n",
    "- [ ] Make multiple PDPs with 1 feature in isolation.\n",
    "- [ ] Make multiple PDPs with 2 features in interaction. \n",
    "- [ ] Use Plotly to make a 3D PDP.\n",
    "- [ ] Make PDPs with categorical feature(s). Use Ordinal Encoder, outside of a pipeline, to encode your data first. If there is a natural ordering, then take the time to encode it that way, instead of random integers. Then use the encoded data with pdpbox. Get readable category names on your plot, instead of integer category codes.\n",
    "\n",
    "#### Shap Values\n",
    "- [ ] Make Shapley force plots to explain at least 4 individual predictions.\n",
    "    - If your project is Binary Classification, you can do a True Positive, True Negative, False Positive, False Negative.\n",
    "    - If your project is Regression, you can do a high prediction with low error, a low prediction with low error, a high prediction with high error, and a low prediction with high error.\n",
    "- [ ] Use Shapley values to display verbal explanations of individual predictions.\n",
    "- [ ] Use the SHAP library for other visualization types.\n",
    "\n",
    "The [SHAP repo](https://github.com/slundberg/shap) has examples for many visualization types, including:\n",
    "\n",
    "- Force Plot, individual predictions\n",
    "- Force Plot, multiple predictions\n",
    "- Dependence Plot\n",
    "- Summary Plot\n",
    "- Summary Plot, Bar\n",
    "- Interaction Values\n",
    "- Decision Plots\n",
    "\n",
    "We just did the first type during the lesson. The [Kaggle microcourse](https://www.kaggle.com/dansbecker/advanced-uses-of-shap-values) shows two more. Experiment and see what you can learn!\n",
    "\n",
    "### Links\n",
    "\n",
    "#### Partial Dependence Plots\n",
    "- [Kaggle / Dan Becker: Machine Learning Explainability — Partial Dependence Plots](https://www.kaggle.com/dansbecker/partial-plots)\n",
    "- [Christoph Molnar: Interpretable Machine Learning — Partial Dependence Plots](https://christophm.github.io/interpretable-ml-book/pdp.html) + [animated explanation](https://twitter.com/ChristophMolnar/status/1066398522608635904)\n",
    "- [pdpbox repo](https://github.com/SauceCat/PDPbox) & [docs](https://pdpbox.readthedocs.io/en/latest/)\n",
    "- [Plotly: 3D PDP example](https://plot.ly/scikit-learn/plot-partial-dependence/#partial-dependence-of-house-value-on-median-age-and-average-occupancy)\n",
    "\n",
    "#### Shapley Values\n",
    "- [Kaggle / Dan Becker: Machine Learning Explainability — SHAP Values](https://www.kaggle.com/learn/machine-learning-explainability)\n",
    "- [Christoph Molnar: Interpretable Machine Learning — Shapley Values](https://christophm.github.io/interpretable-ml-book/shapley.html)\n",
    "- [SHAP repo](https://github.com/slundberg/shap) & [docs](https://shap.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'\n",
    "    \n",
    "# Ignore this Numpy warning when using Plotly Express:\n",
    "# FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning, module='numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('/Users/bradbrauser/Desktop/Data Science/MoviesOnStreamingPlatforms_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16744, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which column in your tabular dataset will you predict, and how is your target distributed?\n",
    "\n",
    "The dataset has two rating features - IMDb and Rotten Tomatoes.\n",
    "\n",
    "IMDb is great for seeing what general audiences think of a movie. If you don’t care what the critics say and want to see what people like yourself think of a movie, then you should use IMDb. Just be aware that fans often skew the vote with 10-star ratings, which may inflate scores somewhat.\n",
    "\n",
    "Rotten Tomatoes offers the best overall picture of whether a movie is worth seeing at a glance. If you only trust the opinions of top critics and just want to know if a movie is at least decent, you should use Rotten Tomatoes. While the Fresh/Rotten binary can oversimplify the often complex opinions of critics, it should still help you weed out lousy films.\n",
    "\n",
    "My goal with this project is more in line with IMDb, as even though scores may be skewed a bit by fans of the movies, I still want to know what the public thinks, because it seems that more often than not critics do not always line up with the public opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571\n",
      "11586\n"
     ]
    }
   ],
   "source": [
    "print(df['IMDb'].isnull().sum())\n",
    "print(df['Rotten Tomatoes'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       int64\n",
       "ID               int64\n",
       "Title           object\n",
       "Year             int64\n",
       "Age             object\n",
       "IMDb           float64\n",
       "Netflix          int64\n",
       "Hulu             int64\n",
       "Prime Video      int64\n",
       "Disney+          int64\n",
       "Type             int64\n",
       "Directors       object\n",
       "Genres          object\n",
       "Country         object\n",
       "Language        object\n",
       "Runtime        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the Rotten Tomatoes features has over 11,000 missing ratings, \n",
    "# I'm going to just drop the Rotten Tomatoes column\n",
    "\n",
    "df = df.drop(['Rotten Tomatoes'], axis = 1)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16173, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['IMDb'], how='all')\n",
    "df['IMDb'].isnull().sum()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = df[(df['Year'] < 2000)]\n",
    "# validate = df[(df['Year'] < 2010) & (df['Year'] >= 2000)]\n",
    "# test = df[df['Year'] >= 2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(df, thresh=500):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Setting Title as index\n",
    "    df.set_index(df['Title'], inplace = True)    \n",
    "    \n",
    "    # Dropping rows if nulls exist\n",
    "    df.dropna(subset=['IMDb'], how='all')\n",
    "    \n",
    "    # Creating new Rating colums\n",
    "    df['Worth Watching?'] = df['IMDb'] >= 6.0\n",
    "        \n",
    "    # Dropping unnecessary values\n",
    "    df.drop(['Unnamed: 0', 'ID', 'Type', 'Title', \n",
    "             'Language', 'IMDb'], \n",
    "            axis=1, inplace=True)\n",
    "    \n",
    "    # Split label and feature matrix\n",
    "    y = df['Worth Watching?']\n",
    "    df.drop(['Worth Watching?'], axis=1, inplace=True)\n",
    "    \n",
    "    return df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Age</th>\n",
       "      <th>Netflix</th>\n",
       "      <th>Hulu</th>\n",
       "      <th>Prime Video</th>\n",
       "      <th>Disney+</th>\n",
       "      <th>Directors</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Country</th>\n",
       "      <th>Runtime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Inception</th>\n",
       "      <td>2010</td>\n",
       "      <td>13+</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>Action,Adventure,Sci-Fi,Thriller</td>\n",
       "      <td>United States,United Kingdom</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Matrix</th>\n",
       "      <td>1999</td>\n",
       "      <td>18+</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Lana Wachowski,Lilly Wachowski</td>\n",
       "      <td>Action,Sci-Fi</td>\n",
       "      <td>United States</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avengers: Infinity War</th>\n",
       "      <td>2018</td>\n",
       "      <td>13+</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Anthony Russo,Joe Russo</td>\n",
       "      <td>Action,Adventure,Sci-Fi</td>\n",
       "      <td>United States</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Back to the Future</th>\n",
       "      <td>1985</td>\n",
       "      <td>7+</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Robert Zemeckis</td>\n",
       "      <td>Adventure,Comedy,Sci-Fi</td>\n",
       "      <td>United States</td>\n",
       "      <td>116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Good, the Bad and the Ugly</th>\n",
       "      <td>1966</td>\n",
       "      <td>18+</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sergio Leone</td>\n",
       "      <td>Western</td>\n",
       "      <td>Italy,Spain,West Germany</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Year  Age  Netflix  Hulu  Prime Video  \\\n",
       "Title                                                                   \n",
       "Inception                       2010  13+        1     0            0   \n",
       "The Matrix                      1999  18+        1     0            0   \n",
       "Avengers: Infinity War          2018  13+        1     0            0   \n",
       "Back to the Future              1985   7+        1     0            0   \n",
       "The Good, the Bad and the Ugly  1966  18+        1     0            1   \n",
       "\n",
       "                                Disney+                       Directors  \\\n",
       "Title                                                                     \n",
       "Inception                             0               Christopher Nolan   \n",
       "The Matrix                            0  Lana Wachowski,Lilly Wachowski   \n",
       "Avengers: Infinity War                0         Anthony Russo,Joe Russo   \n",
       "Back to the Future                    0                 Robert Zemeckis   \n",
       "The Good, the Bad and the Ugly        0                    Sergio Leone   \n",
       "\n",
       "                                                          Genres  \\\n",
       "Title                                                              \n",
       "Inception                       Action,Adventure,Sci-Fi,Thriller   \n",
       "The Matrix                                         Action,Sci-Fi   \n",
       "Avengers: Infinity War                   Action,Adventure,Sci-Fi   \n",
       "Back to the Future                       Adventure,Comedy,Sci-Fi   \n",
       "The Good, the Bad and the Ugly                           Western   \n",
       "\n",
       "                                                     Country  Runtime  \n",
       "Title                                                                  \n",
       "Inception                       United States,United Kingdom    148.0  \n",
       "The Matrix                                     United States    136.0  \n",
       "Avengers: Infinity War                         United States    149.0  \n",
       "Back to the Future                             United States    116.0  \n",
       "The Good, the Bad and the Ugly      Italy,Spain,West Germany    161.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Wrangling\n",
    "X, y = wrangle(df)\n",
    "\n",
    "X.head()\n",
    "# Train test split on years movies were released\n",
    "cutoff = 2010\n",
    "X_train, y_train = X[X['Year'] < cutoff], y[y['Year'] < cutoff]\n",
    "X_val, y_val = X[X['Year'] < cutoff], y[y['Year'] < cutoff]\n",
    "\n",
    "# Baseline\n",
    "y_train.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model 1\n",
    "model1 = Pipeline([\n",
    "    ('oe', OrdinalEncoder()),\n",
    "    ('impute', SimpleImputer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(criterion='entropy', \n",
    "                                          max_depth=50, n_estimators=200, \n",
    "                                          min_samples_leaf=1, \n",
    "                                          random_state=42))\n",
    "])\n",
    "\n",
    "# Fitting the model\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "print('Training Accuracy:', model1.score(X_train, y_train))\n",
    "print('Validation Accuracy:', model1.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model 2\n",
    "model2 = Pipeline([\n",
    "                  ('ohe', OneHotEncoder()),\n",
    "                  ('impute', SimpleImputer()),\n",
    "                  ('scaler', StandardScaler()),\n",
    "                  ('classifier', RandomForestClassifier(criterion='entropy', \n",
    "                                                        max_depth=74, \n",
    "                                                        n_estimators=149, \n",
    "                                                        min_samples_leaf=1, \n",
    "                                                        random_state=42))\n",
    "])\n",
    "\n",
    "# Fitting the model\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "print('Training Accuracy:', model2.score(X_train, y_train))\n",
    "print('Validation Accuracy:', model2.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "model3 = make_pipeline(\n",
    "    ce.OneHotEncoder(use_cat_names=True), \n",
    "    SimpleImputer(), \n",
    "    StandardScaler(), \n",
    "    RandomForestClassifier(criterion='entropy', \n",
    "                           max_depth=74, \n",
    "                           n_estimators=149, \n",
    "                           min_samples_leaf=1, \n",
    "                           random_state=42, \n",
    "                           min_samples_split = 40))\n",
    "\n",
    "# Fitting the model\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "print('Training Accuracy:', model3.score(X_train, y_train))\n",
    "print('Validation Accuracy:', model3.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5\n",
    "model5 = make_pipeline(\n",
    "  OrdinalEncoder(),\n",
    "  SimpleImputer(),\n",
    "  StandardScaler(),\n",
    "  RandomForestClassifier(\n",
    "      min_samples_split=3,\n",
    "      max_depth=15,\n",
    "      n_estimators= 200,\n",
    "      n_jobs=1)\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    'randomforestclassifier__max_depth' : (11, 12, 13, 14, 15),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    model5,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=40,\n",
    "    cv=7,\n",
    "    scoring='accuracy',\n",
    "    verbose = 30,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print('Cross-validation Best Score:', search.best_score_)\n",
    "print('Best Estimator:', search.best_params_)\n",
    "print('Best Model:', search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model6 = make_pipeline(ce.OrdinalEncoder(),\n",
    "                       SimpleImputer(), \n",
    "                       StandardScaler(),\n",
    "                       XGBClassifier(n_estimators=200,\n",
    "                                     random_state=42,\n",
    "                                     n_jobs=-1)\n",
    ")\n",
    "\n",
    "model6.fit(X_train, y_train)\n",
    "\n",
    "print('Training Accuracy:', model6.score(X_train, y_train))\n",
    "print('Validation Accuracy:', model6.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plot_confusion_matrix(model6, X_val, y_val, values_format='.0f', xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model6.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature = 'Netflix'\n",
    "print(X_val[feature].head())\n",
    "print()\n",
    "print(X_val[feature].value_counts())\n",
    "\n",
    "X_val_permuted = X_val.copy()\n",
    "X_val_permuted[feature] = np.random.permutation(X_val_permuted[feature])\n",
    "\n",
    "acc = model6.score(X_val, y_val)\n",
    "acc_permuted = model6.score(X_val_permuted, y_val)\n",
    "\n",
    "print(f'Validation accuracy with {feature}:', acc)\n",
    "print(f'Validation accuracy with {feature} permuted:', acc_permuted)\n",
    "print(f'Permutation importance:', acc - acc_permuted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# Ignore warnings\n",
    "\n",
    "transformers = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median')\n",
    ")\n",
    "\n",
    "X_train_transformed = transformers.fit_transform(X_train)\n",
    "X_val_transformed = transformers.transform(X_val)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=20, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "\n",
    "\n",
    "feature_names = X_val.columns.tolist()\n",
    "\n",
    "permuter = PermutationImportance(\n",
    "    model,\n",
    "    scoring='accuracy',\n",
    "    n_iter=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "permuter.fit(X_val_transformed, y_val)\n",
    "\n",
    "eli5.show_weights(\n",
    "    permuter,\n",
    "    top=None,\n",
    "    feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model 5\n",
    "# model5 = make_pipeline(\n",
    "#   OrdinalEncoder(),\n",
    "#   SimpleImputer(strategy='median'),\n",
    "#   StandardScaler(),\n",
    "#   RandomForestClassifier(\n",
    "#       min_samples_split=3,\n",
    "#       max_depth=15,\n",
    "#       n_estimators= 200,\n",
    "#       n_jobs=1)\n",
    "# )\n",
    "\n",
    "param_distributions = {\n",
    "    'randomforestclassifier__max_depth' : (11, 12, 13, 14, 15),\n",
    "    'randomforestclassifier__min_samples_split': (2, 4, 6, 8, 10),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    model5,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=40,\n",
    "    cv=7,\n",
    "    scoring='accuracy',\n",
    "    verbose = 30,\n",
    "    return_train_score=True,\n",
    "    n_jobs=4,\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print('Cross-validation Best Score:', search.best_score_)\n",
    "print('Best Estimator:', search.best_params_)\n",
    "print('Best Model:', search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# pipeline = make_pipeline(\n",
    "#     ce.OrdinalEncoder(), \n",
    "#     SimpleImputer(), \n",
    "#       StandardScaler(), \n",
    "#     RandomForestRegressor(random_state=42)\n",
    "# )\n",
    "\n",
    "param_distributions = {\n",
    "    'targetencoder__min_samples_leaf': randint(1, 1000),     \n",
    "    'simpleimputer__strategy': ['mean', 'median'], \n",
    "    'randomforestregressor__n_estimators': 15, \n",
    "    'randomforestregressor__max_depth': 14, \n",
    "    'randomforestregressor__max_features': 0.3763983510221083, \n",
    "}\n",
    "\n",
    "search.fit(X_train, y_train);\n",
    "\n",
    "print('Best hyperparameters', search.best_params_)\n",
    "print('Cross-validation MAE', -search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Dependence Plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "processor = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median')\n",
    ")\n",
    "\n",
    "X_train_processed = processor.fit_transform(X_train)\n",
    "X_val_processed = processor.transform(X_val)\n",
    "\n",
    "eval_set = [(X_train_processed, y_train), \n",
    "            (X_val_processed, y_val)]\n",
    "\n",
    "model = XGBClassifier(n_estimators=1000, n_jobs=-1)\n",
    "model.fit(X_train_processed, y_train, eval_set=eval_set, eval_metric='auc', \n",
    "          early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "X_val_processed = processor.transform(X_val)\n",
    "class_index = 1\n",
    "y_pred_proba = model.predict_proba(X_val_processed)[:, class_index]\n",
    "print(f'Test ROC AUC for class {class_index}:')\n",
    "print(roc_auc_score(y_val, y_pred_proba)) # Ranges from 0-1, higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "row_processed = processor.transform(row)\n",
    "shap_values = explainer.shap_values(row_processed)\n",
    "\n",
    "shap.initjs()\n",
    "shap.force_plot(\n",
    "    base_value=explainer.expected_value, \n",
    "    shap_values=shap_values, \n",
    "    features=row, \n",
    "    link='logit' # For classification, this shows predicted probabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = X_val.iloc[[3094]]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model3)\n",
    "row_processed = processor.transform(row)\n",
    "shap_values = explainer.shap_values(row_processed)\n",
    "\n",
    "shap.initjs()\n",
    "shap.force_plot(\n",
    "    base_value=explainer.expected_value, \n",
    "    shap_values=shap_values, \n",
    "    features=row, \n",
    "    link='logit' # For classification, this shows predicted probabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
