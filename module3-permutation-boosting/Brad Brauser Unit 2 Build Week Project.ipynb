{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'\n",
    "    \n",
    "# Ignore this Numpy warning when using Plotly Express:\n",
    "# FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning, module='numpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in movies dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/bradbrauser/Desktop/Data Science/MoviesOnStreamingPlatforms_updated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle function\n",
    "\n",
    "The dataset has two rating features - IMDb and Rotten Tomatoes.\n",
    "\n",
    "IMDb is great for seeing what general audiences think of a movie. If you donâ€™t care what the critics say and want to see what people like yourself think of a movie, then you should use IMDb. Just be aware that fans often skew the vote with 10-star ratings, which may inflate scores somewhat.\n",
    "\n",
    "Rotten Tomatoes offers the best overall picture of whether a movie is worth seeing at a glance. If you only trust the opinions of top critics and just want to know if a movie is at least decent, you should use Rotten Tomatoes. While the Fresh/Rotten binary can oversimplify the often complex opinions of critics, it should still help you weed out lousy films.\n",
    "\n",
    "My goal with this project is more in line with IMDb, as even though scores may be skewed a bit by fans of the movies, I still want to know what the public thinks, because it seems that more often than not critics do not always line up with the public opinion.\n",
    "\n",
    "Not only that, but the Rotten Tomatoes category has over 11,000 null values (IMDb only has less than 600, so with only a little over 16,000 titles, it seemed to be more advantageous to just drop the column all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571\n",
      "11586\n"
     ]
    }
   ],
   "source": [
    "print(df['IMDb'].isnull().sum())\n",
    "print(df['Rotten Tomatoes'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.513514\n",
      "0    0.486486\n",
      "Name: Worth Watching?, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def wrangle(df, thresh=500):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Setting Title as index\n",
    "    df.set_index(df['Title'], inplace = True)\n",
    "    \n",
    "    # Since Rotten Tomatoes feature has over 11,000 missing ratings, I'm going to just drop the Rotten Tomatoes column\n",
    "    df = df.drop(['Rotten Tomatoes'], axis = 1)\n",
    "    \n",
    "    # Dropping rows if nulls exist in IMDb column\n",
    "    df.dropna(subset=['IMDb'], how='all')\n",
    "    \n",
    "    # Creating new target column\n",
    "    df['Worth Watching?'] = df['IMDb'] >= 6.0\n",
    "    \n",
    "    # Creating individual genre columns\n",
    "    df['Action'] = df['Genres'].str.contains('Action')\n",
    "    df['Adventure'] = df['Genres'].str.contains('Adventure')\n",
    "    df['Animation'] = df['Genres'].str.contains('Animation')\n",
    "    df['Biography'] = df['Genres'].str.contains('Biography')\n",
    "    df['Comedy'] = df['Genres'].str.contains('Comedy')\n",
    "    df['Crime'] = df['Genres'].str.contains('Crime')\n",
    "    df['Documentary'] = df['Genres'].str.contains('Documentary')\n",
    "    df['Drama'] = df['Genres'].str.contains('Drama')\n",
    "    df['Family'] = df['Genres'].str.contains('Family')\n",
    "    df['Fantasy'] = df['Genres'].str.contains('Fantasy')\n",
    "    df['Film Noir'] = df['Genres'].str.contains('Film Noir')\n",
    "    df['History'] = df['Genres'].str.contains('History')\n",
    "    df['Horror'] = df['Genres'].str.contains('Horror')\n",
    "    df['Music'] = df['Genres'].str.contains('Music')\n",
    "    df['Musical'] = df['Genres'].str.contains('Musical')\n",
    "    df['Mystery'] = df['Genres'].str.contains('Mystery')\n",
    "    df['Romance'] = df['Genres'].str.contains('Romance')\n",
    "    df['Sci-Fi'] = df['Genres'].str.contains('Sci-Fi')\n",
    "    df['Short Film'] = df['Genres'].str.contains('Short Film')\n",
    "    df['Sport'] = df['Genres'].str.contains('Sport')\n",
    "    df['Superhero'] = df['Genres'].str.contains('Superhero')\n",
    "    df['Thriller'] = df['Genres'].str.contains('Thriller')\n",
    "    df['War'] = df['Genres'].str.contains('War')\n",
    "    df['Western'] = df['Genres'].str.contains('Western')\n",
    "\n",
    "    # Dropping unnecessary values\n",
    "    df.drop(['Genres', 'Unnamed: 0', 'ID', 'Type', 'Title', 'IMDb'], axis=1, inplace=True)\n",
    "    \n",
    "    # Dropping other nulls\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Turning boolean values into binary\n",
    "    df = df*1\n",
    "    \n",
    "    # Split label and feature matrix\n",
    "    # Target\n",
    "    y = df['Worth Watching?']\n",
    "    # Feature\n",
    "    df.drop(['Worth Watching?'], axis=1, inplace=True)\n",
    "    \n",
    "    return df, y\n",
    "\n",
    "# Wrangling data into features and target\n",
    "X, y = wrangle(df)\n",
    "\n",
    "# Baseline - Evaluation metric: Accuracy\n",
    "print(y.value_counts(normalize = True))\n",
    "\n",
    "# Train test split on years movies were released\n",
    "cutoff = 2010\n",
    "X_train = X[X['Year'] < cutoff]\n",
    "y_train = y.loc[X_train.index]\n",
    "X_val = X[X['Year'] > cutoff]\n",
    "y_val = y.loc[X_val.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few comments about the wrangling function above:\n",
    "\n",
    "- I set the 'Title' column as the index because I wanted to be able to see the 'Title' of the movie rather than the index number when doing various slicing.\n",
    "\n",
    "- My question for this project is simply: \"Is this worth watching?\" If you decide to watch a documentary on Netflix, which documentaries are going to be worth the time to sit through?\n",
    "\n",
    "- The target is actually a bit of a combination between the IMDb and Rotten Tomatoes column. I use the IMDb rating as a basis for the target, but make it into a binary classification by setting the lower limit of what is \"Worth Watching?\" to 6.0. My rationale for choosing this lower limit is two-fold:\n",
    "\n",
    "    - In most school systems, the grading scale is based on multiples of 10% (90% = A, 80% = B, etc.). 60%, although seemingly low, is still considered passing.\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Model\n",
    "log_model = Pipeline([\n",
    "                ('oe', OrdinalEncoder()),\n",
    "                ('imputer', SimpleImputer()),\n",
    "                ('classifier', LogisticRegression(random_state = 42, max_iter = 100, \n",
    "                                                  verbose = 5, n_jobs = 4, ))        \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "log_model.fit(X_train, y_train);\n",
    "\n",
    "print('Train accuracy:', log_model.score(X_train, y_train))\n",
    "print('Val accuracy:', log_model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "\n",
    "rf_model = make_pipeline(\n",
    "    ce.OneHotEncoder(), \n",
    "    SimpleImputer(strategy = 'median'), \n",
    "    StandardScaler(), \n",
    "    RandomForestClassifier(criterion='entropy', max_depth=20, n_estimators=77, random_state=42, \n",
    "                           max_features = 0.5234288634835691))\n",
    "\n",
    "param_distributions = {   \n",
    "    'simpleimputer__strategy': ['mean', 'median'], \n",
    "    'randomforestclassifier__n_estimators': randint(50, 500), \n",
    "    'randomforestclassifier__max_depth': [5, 10, 15, 20, None], \n",
    "    'randomforestclassifier__max_features': uniform(0, 1), \n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    rf_model, \n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=10, \n",
    "    cv=3, \n",
    "    scoring='accuracy', \n",
    "    verbose=10, \n",
    "    return_train_score=True, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train);\n",
    "\n",
    "# Fitting the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print('Training Accuracy:', rf_model.score(X_train, y_train))\n",
    "print('Validation Accuracy:', rf_model.score(X_val, y_val))\n",
    "print('Best hyperparameters', search.best_params_)\n",
    "print('Cross-validation MAE', -search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix - Random Forest\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plot_confusion_matrix(rf_model, X_val, y_val, values_format='.0f', xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix - Logistic Regression\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plot_confusion_matrix(log_model, X_val, y_val, values_format='.0f', xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Importances - Random Forest\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(rf_model, X_val, y_val, \n",
    "                                n_repeats=5, random_state=0)\n",
    "\n",
    "df = pd.DataFrame({'feature': X_val.columns,\n",
    "                   'importances_mean': np.round(result['importances_mean'], 3),\n",
    "                   'importances_std': result['importances_std']})\n",
    "\n",
    "df.sort_values(by='importances_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Importances - Logistic Regression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(log_model, X_val, y_val, \n",
    "                                n_repeats=5, random_state=0)\n",
    "\n",
    "df = pd.DataFrame({'feature': X_val.columns,\n",
    "                   'importances_mean': np.round(result['importances_mean'], 3),\n",
    "                   'importances_std': result['importances_std']})\n",
    "\n",
    "df.sort_values(by='importances_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
